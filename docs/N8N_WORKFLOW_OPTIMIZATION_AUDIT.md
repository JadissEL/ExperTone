# Expert Hunter n8n Workflow â€” Pre-Deployment Optimization Audit

**Role:** Principal Automation Architect & Systems Optimization Strategist  
**Date:** 2025-02-19  
**Workflow:** `expert_hunter_cloud_no_env.json`  
**Status:** REFACTOR APPLIED â€” Security fixes implemented; ready for env configuration

---

## Executive Optimization Summary

| Dimension | Current | Target | Gap |
|-----------|---------|--------|-----|
| **Security** | ğŸŸ¢ Fixed | Secrets externalized | âœ… Now uses `$env.GOOGLE_API_KEY`, `$env.GOOGLE_CSE_ID`, `$env.SHARED_SECRET` |
| **Structure** | ğŸŸ¡ Adequate | Elite | Context propagation fragility, redundant logic |
| **Performance** | ğŸŸ¡ Adequate | Optimal | Sequential Google calls, no parallelization |
| **Resilience** | ğŸŸ¢ Improved | Hardened | âœ… Retry on Google Search + POST Callback (2 tries, 1â€“2s wait) |
| **Observability** | ğŸŸ  Weak | Traceable | No execution metadata, sparse logging |
| **Scalability** | ğŸŸ¡ Moderate | 10x-ready | Rate limits, no batching strategy |

**Final Recommendation:** **GO** â€” Critical security and resilience fixes applied. Configure n8n env vars before deploy.

---

## Phase 1 â€” Structural Intelligence Audit

### 1.1 Node Organization & Flow Readability

**Strengths:**
- Clear branching: Collect vs Rank mode
- Fallback paths (Invalid Collect, No Sources, Discarded, No Project)
- Semantic node names (Parse LinkedIn URL, Disambiguate & Aggregate)

**Weaknesses:**

| Issue | Location | Impact |
|-------|----------|--------|
| **Context propagation fragility** | Extract Data, Disambiguate & Aggregate | `$('Parse LinkedIn URL').first().json` and `$('Check robots.txt').first().json` assume single-item or first-item context. With multiple URLs, context can mismatch. |
| **Deep node references** | Format Collect Response | `$('Format Callback').first().json ?? $('Minimal Callback').first().json` â€” brittle; fails if execution path differs. |
| **Merge nodes without clear semantics** | Merge Robots, Merge Context, Merge Scraped | "Merge" is generic; "Combine robots + URL" would be clearer. |

### 1.2 Naming Conventions

| Current | Suggested |
|---------|-----------|
| Merge Google Results | Aggregate Public Search URLs |
| Merge Robots | Combine Robots + URL Context |
| Merge Context | Combine Scraped HTML + URL Metadata |
| No Scrape Fallback | Robots Blocked Fallback |

### 1.3 Redundant / Centralized Logic

- **BLOCKED domains** â€” Hardcoded in Merge Google Results. Move to top-level constant or config.
- **Default expert values** â€” `seniority_score: 50`, `years_experience: 5`, `predicted_rate: 150` repeated in Quality Control and Minimal Callback. Centralize.
- **robots.txt parser** â€” Inline in Check robots.txt. Consider extracting to a reusable snippet.

### 1.4 Suggested Refactor Model

```
Webhook â†’ Extract Input â†’ [Collect? | Rank?]
  Collect: Valid? â†’ Build Queries â†’ [LinkedIn Search] â†’ Parse URL
    â†’ Build Public Queries (Ã—3) â†’ [Public Search] â†’ Aggregate URLs
    â†’ Fetch robots (parallel) â†’ Check â†’ [Allowed?] â†’ Scrape â†’ Extract
    â†’ Disambiguate â†’ Has Sources? â†’ [QC | Minimal Callback]
    â†’ Format â†’ Sign â†’ POST â†’ Respond
  Rank: Has Project? â†’ ML Rank â†’ Format â†’ Respond
```

**Logical grouping:**
1. **Ingest** â€” Webhook, Extract Input
2. **Route** â€” Collect Mode?, Valid Collect?, Has Project?
3. **Search** â€” Build Queries, Google (LinkedIn + Public)
4. **Scrape** â€” Merge URLs, robots, Scrape, Extract
5. **Resolve** â€” Disambiguate, Has Sources?, QC, Minimal Callback
6. **Deliver** â€” Format, Sign, POST, Respond

---

## Phase 2 â€” Performance & Efficiency Review

### 2.1 Unnecessary / Duplicate API Calls

| Call | Count | Optimization |
|------|-------|--------------|
| Google Custom Search (LinkedIn) | 1 per expert | âœ… Necessary |
| Google Custom Search (Public) | 3 per expert (main, interview, conference) | âš ï¸ Could reduce to 1â€“2; "conference speaker" often low yield |
| robots.txt | 1 per URL (up to 20) | âš ï¸ Cache per domain; same domain = same robots |
| Scrape Page | 1 per allowed URL | âœ… Necessary |

### 2.2 Parallelization Opportunities

| Current | Opportunity |
|---------|-------------|
| LinkedIn Search â†’ Parse â†’ Build Public â†’ Public Search | Public Search Ã—3 runs sequentially (one per query). **Parallelize** 3 public queries. |
| Fetch robots.txt | 20 URLs â†’ 20 sequential fetches. **Batch by domain**; fetch each domain's robots once. |
| Scrape Page | After robots check, scrapes are sequential. **Limit concurrency** (e.g. 3â€“5 parallel) to avoid rate limits. |

### 2.3 Payload Bloat

- **Extract Data** passes full `_context` (entire ctx) to each scraped item. Acceptable.
- **Format Callback** strips `_meta` â€” good. Payload is minimal.
- **Merge nodes** â€” Multiplex mode passes all items; no filtering. Consider early filtering of empty items.

### 2.4 Early-Exit Logic

- **Valid Collect Input?** â€” Early exit to Invalid Collect Fallback. âœ…
- **Has Sources?** â€” Early exit to Minimal Callback. âœ…
- **Keep Result?** â€” Early exit to Discarded Fallback. âœ…
- **Missing:** If LinkedIn URL not found, could short-circuit public search (optional optimization).

---

## Phase 3 â€” Resilience & Error Handling Hardening

### 3.1 Failure Mode Map

| Node | Failure Mode | Current Behavior | Risk |
|------|--------------|------------------|------|
| Google Search (LinkedIn) | 429, 500, timeout | No retry; fails workflow | ğŸ”´ High |
| Google Search (Public) | 429, 500, timeout | No retry; fails workflow | ğŸ”´ High |
| Fetch robots.txt | 404, timeout | `allowed()` treats empty as allow | ğŸŸ¡ Medium |
| Scrape Page | 403, timeout | Item fails; Merge continues | ğŸŸ¡ Medium |
| POST Callback | 5xx, timeout | No retry; Vercel never receives | ğŸ”´ High |
| ML Rank Experts | Render cold start | 30s timeout; may fail | ğŸŸ¡ Medium |

### 3.2 Hardening Plan

| Fix | Priority | Implementation |
|-----|----------|----------------|
| **Retry on HTTP 5xx** | P0 | Enable "Retry On Fail" on Google Search, POST Callback (max 2 retries, 5s backoff) |
| **Timeout handling** | P0 | Ensure all HTTP nodes have explicit timeout (already 5â€“15s) |
| **Continue On Fail** | P1 | For Scrape Page: Continue On Fail = true; filter out failed items in Extract Data |
| **Fallback on Google fail** | P1 | If LinkedIn search fails â†’ route to Minimal Callback with `status: 'search_failed'` |
| **Structured error in response** | P2 | On any failure, respond with `{ status: 'error', error: string, code: string }` |

### 3.3 Silent Failure Risks

- **Extract Data** â€” If `$('Check robots.txt').first().json` is undefined (e.g. no items), `ctx` is undefined â†’ potential crash.
- **Disambiguate & Aggregate** â€” If `$('Parse LinkedIn URL').first()` fails (e.g. no output), `ctx` is undefined.
- **Format Collect Response** â€” `$('Format Callback').first()` and `$('Minimal Callback').first()` â€” if neither ran (e.g. Discarded path), expression may fail.

---

## Phase 4 â€” Observability & Debugability

### 4.1 Current State

- **No execution ID** â€” Cannot correlate logs across nodes.
- **No timestamps** â€” Format Collect Response adds `timestamp`; Minimal Callback does not.
- **Sparse logging** â€” Code nodes have no `console.log` or equivalent.
- **Output schema** â€” Varies by path (success vs discarded vs no_sources).

### 4.2 Proposed Improvements

| Improvement | Implementation |
|-------------|----------------|
| **Execution trace ID** | In Extract Input: `const traceId = $execution.id || Date.now().toString(36);` â€” pass through all nodes. |
| **Structured metadata** | Add `_trace: { traceId, path: 'collect'|'rank', phase: string }` to payload. |
| **Log at decision points** | In Code nodes: `console.log(JSON.stringify({ node: 'X', decision: 'Y', value }))` â€” n8n captures this. |
| **Deterministic response schema** | All paths return `{ status, message?, projectId, experts?, error?, traceId? }`. |

### 4.3 Testability

- **Mock payload** â€” Document minimal `{ projectId, expertName }` for Collect; `{ projectId }` for Rank.
- **Mock webhook** â€” Use n8n's "Test workflow" with static JSON.
- **Edge cases** â€” Empty experts array, missing projectId, invalid JSON â€” all have fallbacks. âœ…

---

## Phase 5 â€” Scalability & Future-Proofing

### 5.1 10x Traffic Assumptions

| Assumption | Risk |
|------------|------|
| Google Custom Search: 100 queries/day free tier | At 10x: 1000/day â†’ paid tier required. |
| Single expert per webhook | Batch mode exists (body.experts[]) but not exercised; each expert = full pipeline. |
| Render ML cold start ~30s | At 10x, more concurrent requests â†’ more cold starts. |
| No rate limiting in workflow | External APIs (Google, scraped sites) may throttle. |

### 5.2 Modular Sub-Workflows

| Component | Extract as Sub-Workflow |
|-----------|--------------------------|
| robots.txt check + scrape | "Scrape URL with robots check" â€” reusable. |
| Disambiguate & Aggregate | "Resolve expert identity" â€” reusable for other sources. |
| Format + Sign + POST | "Callback to Vercel" â€” reusable. |

### 5.3 Configuration Abstraction

| Hardcoded | Abstract To |
|-----------|-------------|
| Google API key, cx | `$env.GOOGLE_API_KEY`, `$env.GOOGLE_CSE_ID` |
| HMAC secret | `$env.SHARED_SECRET` or n8n Credential |
| Callback URL | `$env.CALLBACK_URL` or `$env.NEXT_PUBLIC_APP_URL` + path |
| ML Rank URL | `$env.ML_SERVICE_URL` + `/rank` |
| BLOCKED domains | `$env.BLOCKED_DOMAINS` (JSON array) or constant in one place |

---

## Phase 6 â€” Zero-Regret Deployment Check

### 6.1 CRITICAL â€” Block Deployment

| Check | Status | Action |
|-------|--------|--------|
| **Hardcoded Google API key** | ğŸ”´ FAIL | Key `AIzaSyBKBnR0ElJN7wO7-oQI7dommPWp3Crhgd0` exposed. **Rotate immediately.** Use n8n Variables or Credentials. |
| **Hardcoded HMAC secret** | ğŸ”´ FAIL | Secret in Sign Payload. Use `$env.SHARED_SECRET` or n8n secret. |
| **Hardcoded Google cx** | ğŸŸ¡ WARN | `8234580f1acf84e38` â€” move to env. |

### 6.2 Verify Before Deploy

| Check | Status |
|-------|--------|
| No temporary debug nodes | âœ… |
| Webhook URL correct | âœ… `https://exper-tone.vercel.app/api/webhooks/n8n-callback` |
| ML Rank URL | âœ… `https://expertone.onrender.com/rank` |
| Deterministic outputs | âš ï¸ Format Collect Response references two possible nodes |
| Ambiguous conditionals | âœ… Conditions are clear |

### 6.3 Environment Variable Usage

n8n Cloud supports **Workflow Variables** (Settings â†’ Variables). Configure:

```
GOOGLE_API_KEY = <from Google Cloud Console>
GOOGLE_CSE_ID = 8234580f1acf84e38
SHARED_SECRET = <match Vercel SHARED_SECRET>
CALLBACK_URL = https://exper-tone.vercel.app/api/webhooks/n8n-callback
ML_SERVICE_URL = https://expertone.onrender.com
```

---

## Enhancement Opportunities (Ranked by Impact)

| # | Enhancement | Impact | Effort |
|---|-------------|--------|--------|
| 1 | Externalize Google API key + HMAC secret | ğŸ”´ Critical | Low |
| 2 | Add retry on POST Callback (2 retries) | ğŸ”´ High | Low |
| 3 | Add retry on Google Search nodes | ğŸ”´ High | Low |
| 4 | Fix context propagation (Extract Data uses item-specific ctx) | ğŸŸ¡ Medium | Medium |
| 5 | Parallelize 3 public Google queries | ğŸŸ¡ Medium | Medium |
| 6 | Cache robots.txt per domain | ğŸŸ¡ Medium | Medium |
| 7 | Add execution traceId to all paths | ğŸŸ¢ Observability | Low |
| 8 | Centralize default expert values | ğŸŸ¢ Maintainability | Low |
| 9 | Fallback on Google/LinkedIn search failure â†’ Minimal Callback | ğŸŸ¢ Resilience | Medium |

---

## Performance Gains Identified

| Optimization | Est. Gain |
|--------------|-----------|
| Parallelize 3 public queries | ~2x faster collect path |
| Cache robots per domain | Up to 20x fewer robots fetches (if 20 URLs from 2 domains) |
| Early exit when no LinkedIn URL | Skip public search (optional) |
| Limit scrape concurrency | Avoid rate limits; more predictable latency |

---

## Resilience Improvements

| Improvement | Effect |
|-------------|--------|
| Retry POST Callback 2Ã— | Reduces transient Vercel 5xx impact |
| Retry Google Search 2Ã— | Handles 429/500 transient failures |
| Continue On Fail for Scrape | One failed scrape doesn't kill pipeline |
| Fallback on search failure | Graceful degradation to Minimal Callback |

---

## Architectural Refactor Suggestions

1. **Create `expert_hunter_cloud_env.json`** â€” Use `$env.*` for all secrets and URLs. Document required n8n Variables.
2. **Extract "Scrape with robots"** as reusable sub-workflow or shared code.
3. **Add Error Handler** workflow (n8n feature) â€” On any error, log to external service or respond with structured error.
4. **Version workflow** â€” Add `version: "2.0"` in workflow settings for traceability.

---

## Final Recommendation

| Verdict | **GO** |
|---------|-------|
| **Block deployment** until: | n8n env vars configured (GOOGLE_API_KEY, GOOGLE_CSE_ID, SHARED_SECRET). |
| **Recommended before deploy** | Rotate exposed Google API key. |
| **Post-deploy roadmap** | Parallelize queries; cache robots; extract sub-workflows. |

---

## Implementation Status (Applied 2025-02-19)

| Fix | Status |
|-----|--------|
| Replace hardcoded Google API key with `$vars`/`$env` | âœ… Done |
| Replace hardcoded Google cx with `$vars`/`$env` | âœ… Done |
| Replace hardcoded HMAC secret with `$vars`/`$env` | âœ… Done |
| Retry On Fail on all HTTP nodes (Google, robots, scrape, callback, ML rank) | âœ… Done |
| Execution traceId for observability | âœ… Done |
| traceId in all response paths (success + fallbacks) | âœ… Done |
| robots.txt cache per domain (Unique Domains + Combine Robots) | âœ… Done |

## Pre-Deploy Checklist

1. **Configure n8n Variables** â€” The workflow reads from `$vars` (n8n Variables) or `$env` (system env). Set:
   - `GOOGLE_API_KEY` â€” Your Google Custom Search API key
   - `GOOGLE_CSE_ID` â€” Your Custom Search Engine ID (cx)
   - `SHARED_SECRET` â€” Same value as Vercel `SHARED_SECRET` (for HMAC signing)
   
   **n8n Cloud:** Settings â†’ Variables. **Self-hosted:** Environment variables.

2. **Rotate Google API key** â€” The previously hardcoded key was exposed; revoke it in Google Cloud Console and create a new one. Use the new key for `GOOGLE_API_KEY`.

3. **Verify callback URL** â€” Ensure `https://exper-tone.vercel.app/api/webhooks/n8n-callback` (or your app URL) is correct in the POST Callback node.
